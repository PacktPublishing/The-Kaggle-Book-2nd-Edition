{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6e6c8b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip -q install \"copick[all]\" copick-utils zarr matplotlib torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d82629aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Option A: open a locally-synced competition mirror\n",
    "import copick\n",
    "root = copick.from_file(\"/path/to/copick_config.json\")\n",
    "# Option B: open directly from the CZ CryoET Data Portal (dataset id 10440)\n",
    "# This mirrors what the overview uses when exploring portal-hosted data.\n",
    "root = copick.from_czcdp_datasets([10440], overlay_root=\"/tmp/overlay\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5b06aed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Introspection cells from the overview\n",
    "print(\"Pickable objects (name → label id):\")\n",
    "for o in root.pickable_objects:\n",
    "    print(f\" {o.name:>22s} → {o.label}\")\n",
    "\n",
    "print(\"\\nFirst few runs in this project:\")\n",
    "for r in root.runs[:5]:\n",
    "    print(\" \", r.name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7171fd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np, zarr\n",
    "\n",
    "run = root.get_run(root.runs[0].name) # e.g., \"TS_5_4\"\n",
    "vs = run.get_voxel_spacing(10.0) # 10 Å (≈ competition scale)\n",
    "tomo = vs.get_tomogram(\"denoised\") # commonly \"wbp\" in raw form\n",
    "\n",
    "# Zarr stores multiple scales; \"0\" is unbinned, \"1\" is bin-2, etc.\n",
    "tomo_vol = np.array(zarr.open(tomo.zarr())[\"0\"]) # shape: (Z, Y, X)\n",
    "print(\"Tomogram volume:\", tomo_vol.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "840e1a3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read a dense segmentation (semantic labels) into a NumPy array\n",
    "# If multiple users/algorithms exist, choose the one you want to inspect.\n",
    "\n",
    "seg = run.get_segmentations()[0]\n",
    "# same (Z, Y, X) layout as tomo\n",
    "seg_vol = np.array(zarr.open(seg.zarr())[\"0\"])\n",
    "print(\"Segmentation volume:\", seg_vol.shape, seg_vol.dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aaa84898",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "z = tomo_vol.shape[0] // 2 # middle slice\n",
    "fig, (ax0, ax1) = plt.subplots(1, 2, figsize=(10, 5))\n",
    "ax0.imshow(tomo_vol[z], cmap=\"gray\")\n",
    "ax0.set_title(\"Tomogram Slice\")\n",
    "ax0.axis(\"off\")\n",
    "ax1.imshow(seg_vol[z], interpolation=\"nearest\")\n",
    "ax1.set_title(\"Segmentation Mask\")\n",
    "ax1.axis(\"off\")\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d384f38e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build a simple 2D sampling field by summing across z\n",
    "# (For 3D patching you keep it volumetric; here we visualize the footprint.)\n",
    "density_2d = (seg_vol > 0).sum(axis=0).astype(np.float32)\n",
    "density_2d = density_2d / (density_2d.max() + 1e-6) # normalize to [0,1]\n",
    "\n",
    "# Sample patch centers with probability density\n",
    "rng = np.random.default_rng(0)\n",
    "H, W = density_2d.shape\n",
    "ps = 96 # patch size (example)\n",
    "n = 30 # how many rectangles to draw\n",
    "\n",
    "flat_indices = rng.choice(H * W, size=n, p=flat_probs)\n",
    "ys, xs = np.unravel_index(flat_indices, (H, W))\n",
    "\n",
    "# Visualize the sampling plan\n",
    "fig, ax = plt.subplots(figsize=(5, 5))\n",
    "ax.imshow(density_2d, cmap=\"jet\")\n",
    "for y, x in zip(ys, xs):\n",
    "    r0, c0 = max(0, y-ps//2), max(0, x-ps//2)\n",
    "    rect = plt.Rectangle((c0, r0), ps, ps, linewidth=1.5,\n",
    "        edgecolor=\"white\", facecolor=\"none\", alpha=0.7)\n",
    "    ax.add_patch(rect)\n",
    "ax.set_title(\"Patches\")\n",
    "ax.axis(\"off\")\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cae0010",
   "metadata": {},
   "outputs": [],
   "source": [
    "import lightning.pytorch as pl\n",
    "from monai.networks.nets import UNet\n",
    "from monai.losses import TverskyLoss\n",
    "from monai.metrics import DiceMetric\n",
    "\n",
    "class Model(pl.LightningModule):\n",
    "    def __init__(self, spatial_dims=3, in_channels=1, out_channels=7):\n",
    "        super().__init__()\n",
    "        # 3D U-Net with residual units\n",
    "        self.model = UNet(\n",
    "            spatial_dims=spatial_dims,\n",
    "            in_channels=in_channels,\n",
    "            out_channels=out_channels,\n",
    "            channels=(48, 64, 80, 80),\n",
    "            strides=(2, 2, 1),\n",
    "            num_res_units=1\n",
    "        )\n",
    "        # Tversky loss (alpha,beta tuned via include_background)\n",
    "        self.loss_fn = TverskyLoss(\n",
    "            include_background=True,\n",
    "            to_onehot_y=True,\n",
    "            softmax=True\n",
    "        )\n",
    "        # Dice score metric for validation\n",
    "        self.metric_fn = DiceMetric(\n",
    "            include_background=False,\n",
    "            reduction=\"mean\",\n",
    "            ignore_empty=True\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "    \n",
    "    def training_step(self, batch, batch_idx):\n",
    "        x, y = batch['image'], batch['label'] # 3D patch & label mask\n",
    "        y_pred = self(x) # forward pass\n",
    "        loss = self.loss_fn(y_pred, y) # compute Tversky loss\n",
    "        return loss\n",
    "    \n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        x, y = batch['image'], batch['label']\n",
    "        y_pred = self(x)\n",
    "        # Convert predictions & labels to one-hot for Dice metric\n",
    "        pred_onehot = [\n",
    "            AsDiscrete(argmax=True, to_onehot=7)(p)\n",
    "            for p in decollate_batch(y_pred)\n",
    "        ]\n",
    "        label_onehot = [\n",
    "            AsDiscrete(to_onehot=7)(t)\n",
    "            for t in decollate_batch(y)\n",
    "        ]\n",
    "        self.metric_fn(\n",
    "            y_pred=pred_onehot, y=label_onehot) # accumulate Dice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f1f186e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pseudocode for model inference on a tomogram\n",
    "volume = load_volume(experiment_id) # load 3D numpy array for the tomogram\n",
    "    patches, coords = extract_3d_patches(\n",
    "    volume, patch_size=96\n",
    ") # tile the volume\n",
    "\n",
    "mask_preds = np.zeros_like(volume, dtype=np.int8) # initialize empty mask\n",
    "for patch, coord in zip(patches, coords):\n",
    "    patch_tensor = torch.tensor(\n",
    "        patch[None, None, ...]\n",
    "    ).to(device) # shape (1,1,96,96,96)\n",
    "    \n",
    "    output = model(patch_tensor) # output shape (1,7,96,96,96)\n",
    "    probs = torch.softmax(\n",
    "    output[0], dim=0) # 7-channel probability map for the patch\n",
    "    # convert to hard labels:\n",
    "    # pick class with prob > 0.5 (else 0 = background)\n",
    "\n",
    "    # thresholding yields a 7-channel boolean mask\n",
    "    mask_patch = (probs > 0.5).int()\n",
    "\n",
    "    # take the argmax across classes for each voxel\n",
    "    _, pred_class = mask_patch.max(dim=0)\n",
    "\n",
    "    # place patch prediction into full volume mask\n",
    "    mask_preds[coord] = pred_class.cpu().numpy()\n",
    "\n",
    "# Now perform connected components on each class in the full mask:\n",
    "detected_particles = []\n",
    "\n",
    "for class_label in [1,2,3,4,5,6]: # skip 0 (background)\n",
    "    binary_mask = (mask_preds == class_label)\n",
    "    cc = cc3d.connected_components(binary_mask) # label connected regions\n",
    "    stats = cc3d.statistics(cc)\n",
    "\n",
    "    for label_val in range(1, stats['num_objects']+1):\n",
    "        # (z, y, x) in voxel indices\n",
    "        centroid_vox = stats['centroids'][label_val]\n",
    "        size = stats['voxel_counts'][label_val]\n",
    "\n",
    "        if size < BLOB_MIN_SIZE:\n",
    "            continue # skip tiny noise\n",
    "\n",
    "        # Convert voxel coordinates to physical (x,y,z) and\n",
    "        # record detection\n",
    "        centroid_xyz = voxel_to_world(\n",
    "            centroid_vox) # e.g. multiply by voxel size\n",
    "        detected_particles.append(\n",
    "            (experiment_id, class_label, *centroid_xyz)\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffbac65a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e424efa",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34a59868",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb741d92",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ac50da0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcaa38df",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
