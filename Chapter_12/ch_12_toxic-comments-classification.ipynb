{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-01T07:06:15.487996Z",
     "iopub.status.busy": "2025-12-01T07:06:15.487768Z",
     "iopub.status.idle": "2025-12-01T07:06:16.146729Z",
     "shell.execute_reply": "2025-12-01T07:06:16.145989Z",
     "shell.execute_reply.started": "2025-12-01T07:06:15.487973Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from scipy.sparse import hstack"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2025-12-01T07:58:26.882813Z",
     "iopub.status.busy": "2025-12-01T07:58:26.881906Z",
     "iopub.status.idle": "2025-12-01T07:58:28.235524Z",
     "shell.execute_reply": "2025-12-01T07:58:28.234681Z",
     "shell.execute_reply.started": "2025-12-01T07:58:26.882779Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>comment_text</th>\n",
       "      <th>toxic</th>\n",
       "      <th>severe_toxic</th>\n",
       "      <th>obscene</th>\n",
       "      <th>threat</th>\n",
       "      <th>insult</th>\n",
       "      <th>identity_hate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0000997932d777bf</td>\n",
       "      <td>Explanation\\nWhy the edits made under my usern...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>000103f0d9cfb60f</td>\n",
       "      <td>D'aww! He matches this background colour I'm s...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>000113f07ec002fd</td>\n",
       "      <td>Hey man, I'm really not trying to edit war. It...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0001b41b1c6bb37e</td>\n",
       "      <td>\"\\nMore\\nI can't make any real suggestions on ...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0001d958c54c6e35</td>\n",
       "      <td>You, sir, are my hero. Any chance you remember...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 id                                       comment_text  toxic  \\\n",
       "0  0000997932d777bf  Explanation\\nWhy the edits made under my usern...      0   \n",
       "1  000103f0d9cfb60f  D'aww! He matches this background colour I'm s...      0   \n",
       "2  000113f07ec002fd  Hey man, I'm really not trying to edit war. It...      0   \n",
       "3  0001b41b1c6bb37e  \"\\nMore\\nI can't make any real suggestions on ...      0   \n",
       "4  0001d958c54c6e35  You, sir, are my hero. Any chance you remember...      0   \n",
       "\n",
       "   severe_toxic  obscene  threat  insult  identity_hate  \n",
       "0             0        0       0       0              0  \n",
       "1             0        0       0       0              0  \n",
       "2             0        0       0       0              0  \n",
       "3             0        0       0       0              0  \n",
       "4             0        0       0       0              0  "
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train = pd.read_csv('../input/train.csv.zip').fillna(' ')\n",
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-01T07:07:00.918877Z",
     "iopub.status.busy": "2025-12-01T07:07:00.918577Z",
     "iopub.status.idle": "2025-12-01T07:07:03.469538Z",
     "shell.execute_reply": "2025-12-01T07:07:03.468915Z",
     "shell.execute_reply.started": "2025-12-01T07:07:00.918855Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class_names = ['toxic', 'severe_toxic', 'obscene', 'threat',\n",
    "    'insult', 'identity_hate']\n",
    "train = pd.read_csv('train.csv.zip').fillna(' ')\n",
    "test = pd.read_csv('test.csv.zip').fillna(' ')\n",
    "train_text = train['comment_text']\n",
    "test_text = test['comment_text']\n",
    "all_text = pd.concat([train_text, test_text])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-01T07:07:33.480841Z",
     "iopub.status.busy": "2025-12-01T07:07:33.479943Z",
     "iopub.status.idle": "2025-12-01T07:08:00.883395Z",
     "shell.execute_reply": "2025-12-01T07:08:00.882585Z",
     "shell.execute_reply.started": "2025-12-01T07:07:33.480806Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "word_vectorizer = TfidfVectorizer(\n",
    "    sublinear_tf=True,\n",
    "    strip_accents='unicode',\n",
    "    analyzer='word',\n",
    "    token_pattern=r'\\w{1,}',\n",
    "    stop_words='english',\n",
    "    ngram_range=(1, 1),\n",
    "    max_features=10000\n",
    ")\n",
    "word_vectorizer.fit(all_text)\n",
    "train_word_features = word_vectorizer.transform(train_text)\n",
    "test_word_features = word_vectorizer.transform(test_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-01T07:08:00.884893Z",
     "iopub.status.busy": "2025-12-01T07:08:00.884666Z",
     "iopub.status.idle": "2025-12-01T07:18:54.552873Z",
     "shell.execute_reply": "2025-12-01T07:18:54.552235Z",
     "shell.execute_reply.started": "2025-12-01T07:08:00.884870Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/sklearn/feature_extraction/text.py:550: UserWarning: The parameter 'stop_words' will not be used since 'analyzer' != 'word'\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "char_vectorizer = TfidfVectorizer(\n",
    "    sublinear_tf=True,\n",
    "    strip_accents='unicode',\n",
    "    analyzer='char',\n",
    "    ngram_range=(2, 6),\n",
    "    max_features=50000\n",
    ")\n",
    "char_vectorizer.fit(all_text)\n",
    "train_char_features = char_vectorizer.transform(train_text)\n",
    "test_char_features = char_vectorizer.transform(test_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-01T07:18:54.553840Z",
     "iopub.status.busy": "2025-12-01T07:18:54.553609Z",
     "iopub.status.idle": "2025-12-01T07:18:58.089111Z",
     "shell.execute_reply": "2025-12-01T07:18:58.088507Z",
     "shell.execute_reply.started": "2025-12-01T07:18:54.553815Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "train_features = hstack([train_char_features, train_word_features])\n",
    "test_features = hstack([test_char_features, test_word_features])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2025-12-01T07:40:06.110597Z",
     "iopub.status.idle": "2025-12-01T07:40:06.110921Z",
     "shell.execute_reply": "2025-12-01T07:40:06.110772Z",
     "shell.execute_reply.started": "2025-12-01T07:40:06.110758Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "scores = []\n",
    "submission = pd.DataFrame.from_dict({'id': test['id']})\n",
    "for class_name in class_names:\n",
    "    train_target = train[class_name]\n",
    "    classifier = LogisticRegression(C=0.1, solver='sag')\n",
    "    \n",
    "    cv_score = np.mean(cross_val_score(classifier, train_features,\n",
    "        train_target, cv=3, scoring='roc_auc'))\n",
    "    scores.append(cv_score)\n",
    "\n",
    "    print('CV score for class {} is {}'.format(class_name, cv_score))\n",
    "    classifier.fit(train_features, train_target)\n",
    "    submission[class_name] = classifier.predict_proba(test_features)[:, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-01T07:40:08.804922Z",
     "iopub.status.busy": "2025-12-01T07:40:08.804380Z",
     "iopub.status.idle": "2025-12-01T07:40:08.808704Z",
     "shell.execute_reply": "2025-12-01T07:40:08.807909Z",
     "shell.execute_reply.started": "2025-12-01T07:40:08.804901Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total CV score is 0.9802260391883925\n"
     ]
    }
   ],
   "source": [
    "print('Total CV score is {}'.format(np.mean(scores)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-01T07:40:11.853938Z",
     "iopub.status.busy": "2025-12-01T07:40:11.853260Z",
     "iopub.status.idle": "2025-12-01T07:40:12.645769Z",
     "shell.execute_reply": "2025-12-01T07:40:12.645180Z",
     "shell.execute_reply.started": "2025-12-01T07:40:11.853913Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "submission.to_csv('submission.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text preprocessing and cleanup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-01T07:42:28.451510Z",
     "iopub.status.busy": "2025-12-01T07:42:28.450868Z",
     "iopub.status.idle": "2025-12-01T07:42:32.561713Z",
     "shell.execute_reply": "2025-12-01T07:42:32.560961Z",
     "shell.execute_reply.started": "2025-12-01T07:42:28.451484Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting unidecode\n",
      "  Downloading Unidecode-1.4.0-py3-none-any.whl.metadata (13 kB)\n",
      "Downloading Unidecode-1.4.0-py3-none-any.whl (235 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m235.8/235.8 kB\u001b[0m \u001b[31m10.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: unidecode\n",
      "Successfully installed unidecode-1.4.0\n"
     ]
    }
   ],
   "source": [
    "!pip install unidecode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-01T07:42:32.563358Z",
     "iopub.status.busy": "2025-12-01T07:42:32.563077Z",
     "iopub.status.idle": "2025-12-01T07:42:32.572524Z",
     "shell.execute_reply": "2025-12-01T07:42:32.571902Z",
     "shell.execute_reply.started": "2025-12-01T07:42:32.563331Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import os, math, operator, csv, random, pickle,re\n",
    "import gc\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "#from spacy.symbols import nsubj, VERB, dobj\n",
    "import spacy\n",
    "import en_core_web_sm\n",
    "from unidecode import unidecode\n",
    "from sklearn.model_selection import KFold, train_test_split\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "import sys\n",
    "sys.setrecursionlimit(1500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2025-12-01T07:40:25.260540Z",
     "iopub.status.idle": "2025-12-01T07:40:25.260821Z",
     "shell.execute_reply": "2025-12-01T07:40:25.260675Z",
     "shell.execute_reply.started": "2025-12-01T07:40:25.260661Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "with open(hyphens_filepath, mode='rb') as file:\n",
    "    hyphens_dict = pickle.load(file)\n",
    "with open(misspellings_filepath, mode='rb') as file:\n",
    "    misspellings_dict = pickle.load(file)\n",
    "with open(merged_filepath, mode='rb') as file:\n",
    "    merged_dict = pickle.load(file)\n",
    "with open(toxic_words_filepath, mode='rb') as file:\n",
    "    toxic_words = pickle.load(file)\n",
    "with open(asterisk_words_filepath, mode='rb') as file:\n",
    "    asterisk_words = pickle.load(file)\n",
    "with open(fasttext_filepath, mode='rb') as file:\n",
    "    fasttext_misspelings = pickle.load(file)\n",
    "    \n",
    "print(len(hyphens_dict))\n",
    "print(len(misspellings_dict))\n",
    "print(len(merged_dict))\n",
    "print(len(toxic_words))\n",
    "print(len(asterisk_words))\n",
    "print(len(fasttext_misspelings))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-01T08:00:20.398033Z",
     "iopub.status.busy": "2025-12-01T08:00:20.397533Z",
     "iopub.status.idle": "2025-12-01T08:00:20.581009Z",
     "shell.execute_reply": "2025-12-01T08:00:20.580421Z",
     "shell.execute_reply.started": "2025-12-01T08:00:20.398002Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "TEXT_COLUMN = 'comment_text'\n",
    "list_classes = [\"toxic\", \"severe_toxic\", \"obscene\",\n",
    "    \"threat\", \"insult\", \"identity_hate\"]\n",
    "CHARS_TO_REMOVE = \"\"\"!\"#$%&()*+,-./:;<=>?@[\\]^_`{|}~\\t\\n\"\"'∞θ÷α•à−β∅³π‘₹´°£€\\×™√²—\"\"\"\n",
    "\n",
    "submission = pd.read_csv(\"../input/sample_submission.csv.zip\")\n",
    "\n",
    "categories = [\"toxic\", \"severe_toxic\", \"obscene\",\n",
    "    \"threat\", \"insult\", \"identity_hate\"]\n",
    "\n",
    "data_folder = \"../input/\"\n",
    "pretrained_folder = \"../input/\"\n",
    "train_filepath = data_folder + \"train.csv.zip\"\n",
    "test_filepath = data_folder + \"test.csv.zip\"\n",
    "\n",
    "submission_path = data_folder + \"submission.csv\"\n",
    "\n",
    "hyphens_filepath = \"../input/cleaning-dictionaries/hyphens_dictionary.bin\"\n",
    "misspellings_filepath = (\n",
    "\"../input/cleaning-dictionaries/misspellings_all_dictionary.bin\")\n",
    "merged_filepath = \"../input/cleaning-dictionaries/merged_all_dictionary.bin\"\n",
    "toxic_words_filepath = \"../input/cleaning-dictionaries/toxic_words.bin\"\n",
    "asterisk_words_filepath = (\n",
    "\"../input/cleaning-dictionaries/asterisk_words.bin\")\n",
    "fasttext_filepath = \"../input/cleaning-dictionaries/merged_all_dictionary.bin\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-01T08:00:28.258704Z",
     "iopub.status.busy": "2025-12-01T08:00:28.258124Z",
     "iopub.status.idle": "2025-12-01T08:00:28.263035Z",
     "shell.execute_reply": "2025-12-01T08:00:28.262329Z",
     "shell.execute_reply.started": "2025-12-01T08:00:28.258679Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "training_samples_count = 149571\n",
    "validation_samples_count = 10000\n",
    "length_threshold = 20000 # Truncate comments longer than this character length\n",
    "word_count_threshold = 900 # Truncate comments with more than this many words\n",
    "words_limit = 310000\n",
    "valid_characters = (\n",
    "    \" \" + \"@$\" + \"'!?-\" + \"abcdefghijklmnopqrstuvwxyz\"\n",
    "    + \"abcdefghijklmnopqrstuvwxyz\".upper()\n",
    ")\n",
    "valid_characters_ext = valid_characters + \"abcdefghijklmnopqrstuvwxyz\".upper()\n",
    "valid_set = set(x for x in valid_characters)\n",
    "valid_set_ext = set(x for x in valid_characters_ext)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-01T08:00:29.585096Z",
     "iopub.status.busy": "2025-12-01T08:00:29.584506Z",
     "iopub.status.idle": "2025-12-01T08:00:29.590243Z",
     "shell.execute_reply": "2025-12-01T08:00:29.589353Z",
     "shell.execute_reply.started": "2025-12-01T08:00:29.585073Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "cont_patterns = [\n",
    "    (r'(W|w)on\\'t', r'will not'),\n",
    "    (r'(C|c)an\\'t', r'can not'),\n",
    "    (r'(I|i)\\'m', r'i am'),\n",
    "    (r'(A|a)in\\'t', r'is not'),\n",
    "    (r'(\\w+)\\'ll', r'\\g<1> will'),\n",
    "    (r'(\\w+)n\\'t', r'\\g<1> not'),\n",
    "    (r'(\\w+)\\'ve', r'\\g<1> have'),\n",
    "    (r'(\\w+)\\'s', r'\\g<1> is'),\n",
    "    (r'(\\w+)\\'re', r'\\g<1> are'),\n",
    "    (r'(\\w+)\\'d', r'\\g<1> would'),\n",
    "]\n",
    "patterns = [(re.compile(regex), repl) for (regex, repl) in cont_patterns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-01T08:00:36.790466Z",
     "iopub.status.busy": "2025-12-01T08:00:36.789840Z",
     "iopub.status.idle": "2025-12-01T08:00:36.794944Z",
     "shell.execute_reply": "2025-12-01T08:00:36.794349Z",
     "shell.execute_reply.started": "2025-12-01T08:00:36.790442Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def split_word(word, toxic_words):\n",
    "    if word == \"\":\n",
    "        return \"\"\n",
    "        \n",
    "    lower = word.lower()\n",
    "    for toxic_word in toxic_words:\n",
    "        start = lower.find(toxic_word)\n",
    "        if start >= 0:\n",
    "            end = start + len(toxic_word)\n",
    "            result = \" \".join([word[0:start], word[start:end],\n",
    "                split_word(word[end:], toxic_words)])\n",
    "            return result.replace(\" \", \" \").strip()\n",
    "    return word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-01T08:00:37.760353Z",
     "iopub.status.busy": "2025-12-01T08:00:37.759798Z",
     "iopub.status.idle": "2025-12-01T08:00:37.764595Z",
     "shell.execute_reply": "2025-12-01T08:00:37.763767Z",
     "shell.execute_reply.started": "2025-12-01T08:00:37.760327Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "tknzr = TweetTokenizer(strip_handles=False, reduce_len=True)\n",
    "\n",
    "def word_tokenize(sentence):\n",
    "    sentence = sentence.replace(\"$\", \"s\")\n",
    "    sentence = sentence.replace(\"@\", \"a\")\n",
    "    sentence = sentence.replace(\"!\", \" ! \")\n",
    "    sentence = sentence.replace(\"?\", \" ? \")\n",
    "    return tknzr.tokenize(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-01T08:00:38.935912Z",
     "iopub.status.busy": "2025-12-01T08:00:38.935309Z",
     "iopub.status.idle": "2025-12-01T08:00:38.940074Z",
     "shell.execute_reply": "2025-12-01T08:00:38.939160Z",
     "shell.execute_reply.started": "2025-12-01T08:00:38.935889Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def replace_url(word):\n",
    "    if (\"http://\" in word or \"www.\" in word or \"https://\" in word\n",
    "        or \"wikipedia.org\" in word):\n",
    "        return \"\"\n",
    "    return word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-01T08:00:40.015049Z",
     "iopub.status.busy": "2025-12-01T08:00:40.014416Z",
     "iopub.status.idle": "2025-12-01T08:00:40.019774Z",
     "shell.execute_reply": "2025-12-01T08:00:40.018963Z",
     "shell.execute_reply.started": "2025-12-01T08:00:40.015026Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def normalize_by_dictionary(normalized_word, dictionary):\n",
    "    result = []\n",
    "    for word in normalized_word.split():\n",
    "        if word == word.upper():\n",
    "            if word.lower() in dictionary:\n",
    "                result.append(dictionary[word.lower()].upper())\n",
    "            else:\n",
    "                result.append(word)\n",
    "        else:\n",
    "            if word.lower() in dictionary:\n",
    "                result.append(dictionary[word.lower()])\n",
    "            else:\n",
    "                result.append(word)\n",
    "    return \" \".join(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-01T08:00:41.203895Z",
     "iopub.status.busy": "2025-12-01T08:00:41.203150Z",
     "iopub.status.idle": "2025-12-01T08:00:41.932455Z",
     "shell.execute_reply": "2025-12-01T08:00:41.931868Z",
     "shell.execute_reply.started": "2025-12-01T08:00:41.203869Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "nlp = en_core_web_sm.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-01T08:00:42.404514Z",
     "iopub.status.busy": "2025-12-01T08:00:42.403929Z",
     "iopub.status.idle": "2025-12-01T08:00:42.409957Z",
     "shell.execute_reply": "2025-12-01T08:00:42.409405Z",
     "shell.execute_reply.started": "2025-12-01T08:00:42.404488Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def normalize_comment(comment):\n",
    "    comment = unidecode(comment)\n",
    "    comment = comment[:length_threshold]\n",
    "    \n",
    "    # Replace known asterisk patterns\n",
    "    for w in asterisk_words:\n",
    "        if w[0] in comment:\n",
    "            comment = comment.replace(w[0], w[1])\n",
    "        if w[0].upper() in comment:\n",
    "            comment = comment.replace(w[0].upper(), w[1].upper())\n",
    "\n",
    "    normalized_words = []\n",
    "    for word in word_tokenize(comment):\n",
    "        word = replace_url(word)\n",
    "        if word.count(\".\") == 1:\n",
    "            word = word.replace(\".\", \" \")\n",
    "        filtered_word = \"\".join([x for x in word if x in valid_set])\n",
    "\n",
    "        # Split toxic words inside larger tokens\n",
    "        normalized_word = split_word(filtered_word, toxic_words)\n",
    "\n",
    "        # Apply multiple dictionary normalizations\n",
    "        normalized_word = normalize_by_dictionary(\n",
    "        normalized_word, hyphens_dict)\n",
    "        normalized_word = normalize_by_dictionary(\n",
    "        normalized_word, merged_dict)\n",
    "        normalized_word = normalize_by_dictionary(\n",
    "        normalized_word, misspellings_dict)\n",
    "        normalized_word = normalize_by_dictionary(\n",
    "        normalized_word, fasttext_misspelings)\n",
    "        normalized_words.append(normalized_word)\n",
    "    \n",
    "    # Convert words to lowercase unless fully uppercase\n",
    "    normalized_comment = \" \".join(normalized_words)\n",
    "    result = []\n",
    "    for word in normalized_comment.split():\n",
    "        if word.upper() == word:\n",
    "            result.append(word)\n",
    "        else:\n",
    "            result.append(word.lower())\n",
    "    \n",
    "    result = \" \".join(result)\n",
    "    # Merge certain specific words\n",
    "    if \"sock puppet\" in result:\n",
    "    result = result.replace(\"sock puppet\", \"sockpuppet\")\n",
    "    if \"SOCK PUPPET\" in result:\n",
    "    result = result.replace(\"SOCK PUPPET\", \"SOCKPUPPET\")\n",
    "        \n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-01T08:00:43.348002Z",
     "iopub.status.busy": "2025-12-01T08:00:43.347286Z",
     "iopub.status.idle": "2025-12-01T08:00:45.835184Z",
     "shell.execute_reply": "2025-12-01T08:00:45.834254Z",
     "shell.execute_reply.started": "2025-12-01T08:00:43.347975Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'normalize_comment' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_47/3323624484.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[0;31m# Execute the function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 33\u001b[0;31m \u001b[0mlabels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx_test\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mread_data_files\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_filepath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_filepath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/tmp/ipykernel_47/3323624484.py\u001b[0m in \u001b[0;36mread_data_files\u001b[0;34m(train_filepath, test_filepath)\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0;31m# 3. Setup Normalization\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[0;31m# Assumes 'normalize_comment' is defined globally\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m     \u001b[0mnp_normalize\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvectorize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnormalize_comment\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m     \u001b[0;31m# 4. Process Train Comments\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'normalize_comment' is not defined"
     ]
    }
   ],
   "source": [
    "def read_data_files(train_filepath, test_filepath):\n",
    "    # read train data\n",
    "    train = pd.read_csv(train_filepath)\n",
    "    labels = train[categories].values\n",
    "\n",
    "    # read test data\n",
    "    test = pd.read_csv(test_filepath)\n",
    "    test_comments = test[\"comment_text\"].fillna(\"_na_\").values\n",
    "\n",
    "    # normalize comments\n",
    "    np_normalize = np.vectorize(normalize_comment)\n",
    "    comments = train[\"comment_text\"].fillna(\"_na_\").values\n",
    "    normalized_comments = np_normalize(comments)\n",
    "    del comments\n",
    "    gc.collect()\n",
    "\n",
    "    comments = test[\"comment_text\"].fillna(\"_na_\").values\n",
    "    normalized_test_comments = np_normalize(test_comments)\n",
    "    del comments\n",
    "    gc.collect()\n",
    "\n",
    "    print('Shape of data tensor:', normalized_comments.shape)\n",
    "    print('Shape of label tensor:', labels.shape)\n",
    "    print('Shape of test data tensor:', normalized_test_comments.shape)\n",
    "\n",
    "    return (labels, normalized_comments, normalized_test_comments)\n",
    "\n",
    "labels, x_train, x_test = read_data_files(train_filepath, test_filepath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2025-12-01T08:00:45.835621Z",
     "iopub.status.idle": "2025-12-01T08:00:45.835853Z",
     "shell.execute_reply": "2025-12-01T08:00:45.835755Z",
     "shell.execute_reply.started": "2025-12-01T08:00:45.835745Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "np.save(\"../cleaned_data/lables\", labels)\n",
    "np.save(\"../cleaned_data/x_train\", x_train)\n",
    "np.save(\"../cleaned_data/x_test\", x_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text classification with RNNs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2025-12-01T07:40:06.132899Z",
     "iopub.status.idle": "2025-12-01T07:40:06.133137Z",
     "shell.execute_reply": "2025-12-01T07:40:06.133041Z",
     "shell.execute_reply.started": "2025-12-01T07:40:06.133023Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import os, math, operator, csv, random, pickle, re\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import gc\n",
    "import tensorflow as tf\n",
    "\n",
    "print(tf.__version__)\n",
    "tf.test.is_gpu_available(\n",
    "    cuda_only=False,\n",
    "    min_cuda_compute_capability=None\n",
    ")\n",
    "\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import (\n",
    "    MaxPooling1D, BatchNormalization, Permute, Lambda, Activation, Conv1D,\n",
    "    GlobalAveragePooling1D, GlobalMaxPooling1D, Dense, Embedding, Dropout,\n",
    "    Input, Flatten, TimeDistributed, concatenate, SpatialDropout1D,\n",
    "    Bidirectional, LSTM, GRU, add\n",
    ")\n",
    "from tensorflow.keras.callbacks import LearningRateScheduler\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras import backend as K\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "from unidecode import unidecode\n",
    "from sklearn.model_selection import KFold, train_test_split\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Load all the preprocessed data as numpy text arrays.\n",
    "labels = np.load('../input/labels.npy')\n",
    "x_train = np.load('../input/x_train.npy')\n",
    "x_test = np.load('../input/x_test.npy')\n",
    "fileObject = open('../dictionaries/tokenizer','rb')\n",
    "tokenizer = pickle.load(fileObject)\n",
    "\n",
    "x_train = tokenizer.texts_to_sequences(x_train)\n",
    "x_test = tokenizer.texts_to_sequences(x_test)\n",
    "x_train = sequence.pad_sequences(x_train, maxlen=MAX_LEN)\n",
    "x_test = sequence.pad_sequences(x_test, maxlen=MAX_LEN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Load the dual embeddings matrix:\n",
    "embedding_matrix = np.load('../embeddings/embedding_matrix_big.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# split the train data into the train and validation sets\n",
    "x_train, x_valid, y_train, y_valid = train_test_split(\n",
    "x_train, labels, test_size = 0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Define the Keras model\n",
    "def build_model(embedding_matrix):\n",
    "    words = Input(shape=(None,))\n",
    "    \n",
    "    # Embedding Layer\n",
    "    x = Embedding(*embedding_matrix.shape,\n",
    "                  weights=[embedding_matrix],\n",
    "                  trainable=False)(words)\n",
    "    x = SpatialDropout1D(0.2)(x)\n",
    "    \n",
    "    # Recurrent Layers\n",
    "    x = Bidirectional(GRU(LSTM_UNITS, return_sequences=True))(x)\n",
    "    x = Bidirectional(LSTM(LSTM_UNITS, return_sequences=True))(x)\n",
    "    \n",
    "    # Pooling & Concatenation\n",
    "    hidden = concatenate([\n",
    "        GlobalMaxPooling1D()(x),\n",
    "        GlobalAveragePooling1D()(x),\n",
    "    ])\n",
    "    \n",
    "    # Residual Connections (Skip Connections)\n",
    "    hidden = add([\n",
    "        hidden,\n",
    "        Dense(DENSE_HIDDEN_UNITS, activation='relu')(hidden)\n",
    "    ])\n",
    "    hidden = add([\n",
    "        hidden,\n",
    "        Dense(DENSE_HIDDEN_UNITS, activation='relu')(hidden)\n",
    "    ])\n",
    "    \n",
    "    # Output Layer\n",
    "    result = Dense(6, activation='sigmoid')(hidden)\n",
    "    \n",
    "    model = Model(inputs=words, outputs=result)\n",
    "    model.compile(loss='binary_crossentropy', optimizer='adam')\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model and make predictions on the test set.\n",
    "# In order to improve performance we use a 10 seed average.\n",
    "EPOCHS = 5\n",
    "SEEDS = 10\n",
    "pred = 0\n",
    "for ii in range(SEEDS):\n",
    "    model = build_model(embedding_matrix)\n",
    "    for global_epoch in range(EPOCHS):\n",
    "        print(global_epoch)\n",
    "        model.fit(\n",
    "            x_train,\n",
    "            y_train,\n",
    "            validation_data = (x_valid, y_valid),\n",
    "            batch_size=128,\n",
    "            epochs=1,\n",
    "            verbose=2,\n",
    "            callbacks=[LearningRateScheduler(\n",
    "                lambda _: 1e-3 * (0.55 ** global_epoch))\n",
    "            ]\n",
    "        )\n",
    "        val_preds = model.predict(x_valid)\n",
    "        AUC = 0\n",
    "        for i in range(6):\n",
    "            AUC += roc_auc_score(y_valid[:,i], val_preds[:,i])/6.\n",
    "        print(AUC)\n",
    "\n",
    "pred += model.predict(x_test, batch_size = 1024, verbose = 1)/SEEDS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We create the submission file\n",
    "list_classes = [\"toxic\", \"severe_toxic\", \"obscene\",\n",
    "\"threat\", \"insult\", \"identity_hate\"]\n",
    "submission = pd.read_csv('../input/sample_submission.csv')\n",
    "submission[list_classes] = pred\n",
    "submission.to_csv('../submissions/submission.csv', index=False)\n",
    "submission.head()\n",
    "# This model scores 0.98644 on the Private Leaderboard, and 0.98653 on the public leaderboard"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text classification with DistilBERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "from torch.utils.data import Dataset, DataLoader, RandomSampler, SequentialSampler\n",
    "from transformers import DistilBertTokenizer, DistilBertModel\n",
    "MAX_LEN = 320\n",
    "TRAIN_BATCH_SIZE = 32\n",
    "VALID_BATCH_SIZE = 32\n",
    "EPOCHS = 2\n",
    "LEARNING_RATE = 1e-05\n",
    "DEVICE = 'cuda:0' if torch.cuda.is_available() else 'cpu'\n",
    "print(DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = pd.read_csv('../input/train.csv.zip')\n",
    "label_columns = [\"toxic\", \"severe_toxic\", \"obscene\",\n",
    "    \"threat\", \"insult\", \"identity_hate\"]\n",
    "\n",
    "train_data['labels'] = train_data[label_columns].apply(lambda x: list(x), axis=1)\n",
    "train_data.drop(['id'], inplace=True, axis=1)\n",
    "train_data.drop(label_columns, inplace=True, axis=1)\n",
    "print(train_data.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiLabelDataset(Dataset):\n",
    "    def __init__(self, dataframe, tokenizer, max_len, new_data=False):\n",
    "        self.tokenizer = tokenizer\n",
    "        self.data = dataframe\n",
    "        self.text = dataframe.comment_text\n",
    "        self.new_data = new_data\n",
    "\n",
    "        if not new_data:\n",
    "            self.targets = self.data.labels\n",
    "        self.max_len = max_len\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.text)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        text = str(self.text[index])\n",
    "        text = \" \".join(text.split())\n",
    "        inputs = self.tokenizer.encode_plus(\n",
    "            text,\n",
    "            None,\n",
    "            add_special_tokens=True,\n",
    "            max_length=self.max_len,\n",
    "            pad_to_max_length=True,\n",
    "            return_token_type_ids=True\n",
    "        )\n",
    "        ids = inputs['input_ids']\n",
    "        mask = inputs['attention_mask']\n",
    "        token_type_ids = inputs[\"token_type_ids\"]\n",
    "        out = {\n",
    "            'ids': torch.tensor(ids, dtype=torch.long),\n",
    "            'mask': torch.tensor(mask, dtype=torch.long),\n",
    "            'token_type_ids': torch.tensor(token_type_ids,\n",
    "            dtype=torch.long),\n",
    "        }\n",
    "        if not self.new_data:\n",
    "            out['targets'] = torch.tensor(self.targets[index],\n",
    "                dtype=torch.float)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_size = 1.0\n",
    "\n",
    "train_df = train_data.sample(frac=train_size, random_state=123)\n",
    "val_df = train_data.drop(train_df.index).reset_index(drop=True)\n",
    "train_df = train_df.reset_index(drop=True)\n",
    "\n",
    "print(\"Orig Dataset: {}\".format(train_data.shape))\n",
    "print(\"Training Dataset: {}\".format(train_df.shape))\n",
    "print(\"Validation Dataset: {}\".format(val_df.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = DistilBertTokenizer.from_pretrained(\n",
    "    'distilbert-base-uncased', truncation=True, do_lower_case=True)\n",
    "\n",
    "training_set = MultiLabelDataset(train_df, tokenizer, MAX_LEN)\n",
    "val_set = MultiLabelDataset(val_df, tokenizer, MAX_LEN)\n",
    "\n",
    "train_params = {'batch_size': TRAIN_BATCH_SIZE,\n",
    "    'shuffle': True,\n",
    "    'num_workers': 8}\n",
    "\n",
    "val_params = {'batch_size': VALID_BATCH_SIZE,\n",
    "    'shuffle': False,\n",
    "    'num_workers': 8}\n",
    "\n",
    "training_loader = DataLoader(training_set, **train_params)\n",
    "#val_loader = DataLoader(val_set, **val_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DistilBERTClass(torch.nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super(DistilBERTClass, self).__init__()\n",
    "        self.bert = DistilBertModel.from_pretrained(\"distilbert-base-uncased\")\n",
    "        self.classifier = torch.nn.Sequential(\n",
    "            torch.nn.Linear(768, 768),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Dropout(0.1),\n",
    "            torch.nn.Linear(768, 6)\n",
    "        )\n",
    "\n",
    "    def forward(self, input_ids, attention_mask, token_type_ids):\n",
    "        output_1 = self.bert(input_ids=input_ids,\n",
    "            attention_mask=attention_mask)\n",
    "        hidden_state = output_1[0]\n",
    "        out = hidden_state[:, 0] # [CLS] token representation\n",
    "        out = self.classifier(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = DistilBERTClass()\n",
    "model.to(DEVICE)\n",
    "optimizer = torch.optim.Adam(params=model.parameters(), lr=LEARNING_RATE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(epoch):\n",
    "    model.train()\n",
    "\n",
    "    for _, data in tqdm(enumerate(training_loader, 0)):\n",
    "        ids = data['ids'].to(DEVICE, dtype=torch.long)\n",
    "        mask = data['mask'].to(DEVICE, dtype=torch.long)\n",
    "        token_type_ids = data[\n",
    "            'token_type_ids'].to(DEVICE, dtype=torch.long)\n",
    "        targets = data['targets'].to(DEVICE, dtype=torch.float)\n",
    "\n",
    "        outputs = model(ids, mask, token_type_ids)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss = torch.nn.functional.binary_cross_entropy_with_logits(\n",
    "            outputs, targets)\n",
    "        \n",
    "        if _ % 5000 == 0:\n",
    "            print(f'Epoch: {epoch}, Loss: {loss.item()}')\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    train(epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data = pd.read_csv('../input/test.csv.zip')\n",
    "print(test_data.head())\n",
    "\n",
    "test_set = MultiLabelDataset(test_data, tokenizer, MAX_LEN, new_data=True)\n",
    "test_loader = DataLoader(test_set, **val_params)\n",
    "\n",
    "all_test_pred = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(epoch):\n",
    "    model.eval()\n",
    "\n",
    "    with torch.inference_mode():\n",
    "        for _, data in tqdm(enumerate(test_loader, 0)):\n",
    "        ids = data['ids'].to(DEVICE, dtype=torch.long)\n",
    "        mask = data['mask'].to(DEVICE, dtype=torch.long)\n",
    "        token_type_ids = data[\n",
    "            'token_type_ids'].to(DEVICE, dtype=torch.long)\n",
    "        outputs = model(ids, mask, token_type_ids)\n",
    "        probas = torch.sigmoid(outputs)\n",
    "\n",
    "        all_test_pred.append(probas)\n",
    "\n",
    "    return probas\n",
    "\n",
    "probas = test(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_test_pred = torch.cat(all_test_pred)\n",
    "\n",
    "submit_df = test_data.copy()\n",
    "submit_df.drop(\"comment_text\", inplace=True, axis=1)\n",
    "\n",
    "label_columns = [\"toxic\", \"severe_toxic\", \"obscene\",\n",
    "    \"threat\", \"insult\", \"identity_hate\"]\n",
    "\n",
    "for i,name in enumerate(label_columns):\n",
    "    submit_df[name] = all_test_pred[:, i].cpu()\n",
    "\n",
    "submit_df.to_csv('../submissions/distilbert_0.csv', index=False)\n",
    "\n",
    "submit_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text classification with AutoTrain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "import os\n",
    "\n",
    "from kaggle_secrets import UserSecretsClient\n",
    "user_secrets = UserSecretsClient()\n",
    "HF_USERNAME = \"tunguz\"\n",
    "HF_TOKEN = user_secrets.get_secret(\"HUGGING_FACE_HUB_TOKEN\")\n",
    "\n",
    "from autotrain.params import TextClassificationParams\n",
    "from autotrain.project import AutoTrainProject\n",
    "\n",
    "import torch\n",
    "from sklearn import model_selection, metrics\n",
    "from transformers import (\n",
    "    AutoModelForSequenceClassification,\n",
    "    AutoTokenizer, TrainingArguments, Trainer\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv(‘../input/toxic-train/train.csv’)\n",
    "test = pd.read_csv('../input/toxic-train/test.csv')\n",
    "print(train.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = TextClassificationParams(\n",
    "    model=\"google-bert/bert-base-uncased\",\n",
    "    data_path=\"../input/toxic-train/\",\n",
    "    text_column=\"comment_text\",\n",
    "    target_column=\"toxic\",\n",
    "    train_split=\"train\",\n",
    "    valid_split=None,\n",
    "    epochs=3,\n",
    "    batch_size=8,\n",
    "    max_seq_length=512,\n",
    "    lr=1e-5,\n",
    "    optimizer=\"adamw_torch\",\n",
    "    scheduler=\"linear\",\n",
    "    gradient_accumulation=1,\n",
    "    mixed_precision=\"fp16\",\n",
    "    project_name=\"autotrain-model\",\n",
    "    log=\"tensorboard\",\n",
    "    push_to_hub=False,\n",
    "    username=HF_USERNAME,\n",
    "    token=HF_TOKEN,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "project = AutoTrainProject(params=params, backend=\"local\", process=True)\n",
    "project.create()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    \"../input/toxic-autotrain-toxic/autotrain-model\", use_fast=True)\n",
    "\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    \"../input/toxic-autotrain-toxic/autotrain-model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test.loc[:, \"toxic\"] = 0\n",
    "test.loc[:, \"severe_toxic\"] = 0\n",
    "test.loc[:, \"obscene\"] = 0\n",
    "test.loc[:, \"threat\"] = 0\n",
    "test.loc[:, \"insult\"] = 0\n",
    "test.loc[:, \"identity_hate\"] = 0\n",
    "print(test.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ClassificationDataset:\n",
    "\n",
    "    def __init__(self, data, tokenizer):\n",
    "        self.data = data\n",
    "        self.tokenizer = tokenizer\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, item):\n",
    "        text = str(self.data[\"comment_text\"].values[item])\n",
    "        target = int(self.data[\"toxic\"].values[item])\n",
    "        inputs = self.tokenizer(\n",
    "            text,\n",
    "            max_length=512,\n",
    "            padding=\"max_length\",\n",
    "            truncation=True\n",
    "        )\n",
    "        ids = inputs[\"input_ids\"]\n",
    "        mask = inputs[\"attention_mask\"]\n",
    "        return {\n",
    "            \"input_ids\": torch.tensor(ids, dtype=torch.long),\n",
    "            \"attention_mask\": torch.tensor(mask, dtype=torch.long),\n",
    "            \"labels\": torch.tensor(target, dtype=torch.long),\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = ClassificationDataset(test, tokenizer)\n",
    "trainer = Trainer(model)\n",
    "preds = trainer.predict(dataset).predictions\n",
    "# Preds will be in the form of logits,\n",
    "# and need to be converted into probabilities before submission."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## OpenAI embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import openai\n",
    "from openai import OpenAI\n",
    "\n",
    "client = OpenAI()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_embedding(text, model=\"text-embedding-3-large\"):\n",
    "    text = text.replace(\"\\n\", \" \")\n",
    "    return client.embeddings.create(\n",
    "    input = [text], model=model\n",
    "    ).data[0].embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv('../input/train.csv.zip').fillna(' ')[['comment_text']]\n",
    "test = pd.read_csv('../input/test.csv.zip').fillna(' ')[['comment_text']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test.at[9932, 'comment_text'] = '*'\n",
    "test.at[55331, 'comment_text'] = '*'\n",
    "test.at[97708, 'comment_text'] = '*'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train['embedding_3_large'] = train.comment_text.apply(\n",
    "    lambda x: get_embedding(x, model='text-embedding-3-large'))\n",
    "\n",
    "test['embedding_3_large'] = test.comment_text.apply(\n",
    "    lambda x: get_embedding(x, model='text-embedding-3-large'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_embeds = np.array(\n",
    "    [np.array(i) for i in train.embedding_3_large.values])\n",
    "\n",
    "test_embeds = np.array(\n",
    "    [np.array(i) for i in test.embedding_3_large.values])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save('../input/test_embs_3_large', test_embeds)\n",
    "np.save('../input/train_embs_3_large', train_embeds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NVIDIA embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModel\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "model = AutoModel.from_pretrained('nvidia/NV-Embed-v2', trust_remote_code=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_embedding(text):\n",
    "    text = text.replace(\"\\n\", \" \")\n",
    "    return model.encode([text], instruction=passage_prefix,\n",
    "        max_length=max_length)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "class_names = ['toxic', 'severe_toxic', 'obscene',\n",
    "    'threat', 'insult', 'identity_hate']\n",
    "target = pd.read_csv('../input/train.csv.zip').fillna(' ')[class_names].values\n",
    "train_features = np.load('../input/train_embs_NV_2.npy')\n",
    "test_features = np.load('../input/test_embs_NV_2.npy')\n",
    "sample_submission = pd.read_csv('../input/sample_submission.csv.zip')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = np.zeros((test_features.shape[0], target.shape[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Cs = [4, 1, 4, 3, 2, 2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "errors = []\n",
    "train_oof = np.zeros(target.shape)\n",
    "kf = KFold(n_splits=5, random_state=137, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for ii in range(6):\n",
    "    print(\"Fitting target\", ii+1)\n",
    "    for jj, (\n",
    "        train_index, val_index) in enumerate(kf.split(train_features)\n",
    "        ):\n",
    "        print(\"Fitting fold\", jj+1)\n",
    "        train_x = train_features[train_index]\n",
    "        val_x = train_features[val_index]\n",
    "        train_target = target[train_index, ii]\n",
    "        classifier = LogisticRegression(C=Cs[ii], solver='sag', max_iter=10)\n",
    "        classifier.fit(train_x, train_target)\n",
    "\n",
    "        train_oof[val_index, ii] = classifier.predict_proba(val_x)[:,1]\n",
    "        preds[:, ii] += classifier.predict_proba(test_features)[:,1]/5\n",
    "        train_target = target[train_index, ii]\n",
    "\n",
    "        print(roc_auc_score(target[:,ii], train_oof[:,ii]))\n",
    "        errors.append(roc_auc_score(target[:,ii], train_oof[:,ii]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_submission[class_names] = preds\n",
    "sample_submission.to_csv('../input/NV_2_LR.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "databundleVersionId": 44219,
     "sourceId": 8076,
     "sourceType": "competition"
    }
   ],
   "dockerImageVersionId": 31193,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
